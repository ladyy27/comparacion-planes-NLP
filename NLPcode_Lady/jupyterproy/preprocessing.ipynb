{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing Jupyter notebook from RDFmanagement.ipynb\n"
     ]
    }
   ],
   "source": [
    "import nbimporter\n",
    "import codecs\n",
    "#from RDFmanagement import *\n",
    "from rdflib import Graph, Literal, BNode, Namespace, RDF,RDFS, URIRef\n",
    "from spacy.tokens import Span\n",
    "from RDFmanagement import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopwordsList(filename):\n",
    "    #Cargar lista de stopwords\n",
    "    stopwordslist = []\n",
    "    stopfile= codecs.open(filename,\"r\",encoding=\"UTF-8\")\n",
    "    for line in stopfile:\n",
    "        stop = line\n",
    "        stop2 = stop.replace(\"\\n\",\"\")\n",
    "        stopwordslist.append(stop2)\n",
    "    return stopwordslist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_tildes(es_string):\n",
    "    es_string1 = es_string.replace(\"á\", \"a\").replace(\"é\", \"e\").replace(\"í\", \"i\").replace(\"ó\", \"o\").replace(\"ú\", \"u\").replace(\"Á\", \"A\").replace(\"É\", \"E\").replace(\"Í\", \"I\").replace(\"Ó\", \"O\").replace(\"Ú\", \"U\")\n",
    "    return es_string1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateTriplesByDetectedLang(doc,graph, sentenceNode,lang_str,idtoken, idsentence, stoplist_lang, stoplist_punt):\n",
    "    # RDF: Seteando propiedades y clases\n",
    "    vutplSchema = Namespace('http://data.utpl.edu.ec/vutpl/vocabulary/')\n",
    "    vutplResource = Namespace('http://data.utpl.edu.ec/vutpl/resource/')\n",
    "\n",
    "    sentencetype = URIRef(vutplSchema+'Sentence')\n",
    "    tokentype = URIRef(vutplSchema+'Token')\n",
    "    roltype = URIRef(vutplSchema+'Rol')\n",
    "    chunktype = URIRef(vutplSchema+'Chunk')\n",
    "    dependencytype = URIRef(vutplSchema+'Dependency')\n",
    "    postagtype = URIRef(vutplSchema+'PosTag')\n",
    "    entitytype = URIRef(vutplSchema+'Entity')\n",
    "\n",
    "    #Definir Clases\n",
    "    grade = URIRef(vutplSchema+'Grade/')\n",
    "    subject = URIRef(vutplSchema+'Subject/')\n",
    "    plan = URIRef(vutplSchema+'TeachingPlan/')\n",
    "    section = URIRef(vutplSchema+'Section/')\n",
    "    sentence = URIRef(vutplResource+'Sentence/')\n",
    "    tokenURI= URIRef(vutplResource+'Token/')\n",
    "    rol = URIRef(vutplResource+'Rol/')\n",
    "    chunk = URIRef(vutplResource+'Chunk/')\n",
    "    dependency = URIRef(vutplResource+'Dependency/')\n",
    "    postag = URIRef(vutplResource+'PosTag/')\n",
    "    entity = URIRef(vutplResource+'Entity/')\n",
    "\n",
    "    #Definir Propiedades\n",
    "    hasSubject = URIRef(vutplSchema+'hasSubject')\n",
    "    hasTeachingPlan = URIRef(vutplSchema+'hasTeachingPlan')\n",
    "    hasSection = URIRef(vutplSchema+'hasSection')\n",
    "    hasSentence = URIRef(vutplSchema+'hasSentence')\n",
    "    sentenceId = URIRef(vutplSchema+'sentenceId')\n",
    "    dataSentence = URIRef(vutplSchema+'dataSentence')\n",
    "    hasToken= URIRef(vutplSchema+'hasToken')\n",
    "    pnp = URIRef(vutplSchema+'pnp')\n",
    "    tokenId = URIRef(vutplSchema+'tokenId')\n",
    "    pos = URIRef(vutplSchema+'pos')\n",
    "    data = URIRef(vutplSchema+'data')\n",
    "    ner = URIRef(vutplSchema+'ner')\n",
    "    nertype = URIRef(vutplSchema+'nertype')\n",
    "    lemma = URIRef(vutplSchema+'lemma')\n",
    "    hasRole = URIRef(vutplSchema+'hasRole')\n",
    "    hasChunk = URIRef(vutplSchema+'hasChunk')\n",
    "    \n",
    "    for sent in doc.sents:\n",
    "        for token in sent:\n",
    "            if token.lemma_ not in stoplist_lang:\n",
    "                if token.lemma_ not in stoplist_punt:\n",
    "                    tokenNode = add_node(tokenURI+\"Token\" + str(idtoken) + \"_Sentence\" + str(idsentence))\n",
    "                    add_graph(graph, tokenNode, RDF.type, tokentype)\n",
    "                    add_graph(graph, sentenceNode, hasToken, URIRef(tokenNode))\n",
    "                    add_graph(graph, tokenNode, tokenId, Literal(idtoken, lang=lang_str))\n",
    "                    posTagNode = add_node(postag + token.pos_)\n",
    "                    add_graph(graph, posTagNode, RDF.type, postagtype)\n",
    "                    add_graph(graph, posTagNode, RDFS.label, Literal(token.pos_, lang=lang_str))\n",
    "                    add_graph(graph, tokenNode, pos, posTagNode)\n",
    "                    add_graph(graph, tokenNode, RDFS.label, Literal(token.pos_, lang=lang_str))\n",
    "                    add_graph(graph, tokenNode, data, Literal(token.text, lang=lang_str))\n",
    "                    add_graph(graph, tokenNode, lemma, Literal(token.lemma_, lang=lang_str))\n",
    "                    #print(token.text, token.lemma_, token.pos_,token.is_stop)\n",
    "                    idtoken += 1\n",
    "        idtoken = 1\n",
    "        idsentence += 1\n",
    "    #for ent in doc.ents:\n",
    "    #entityNode = add_node(entity + ent.label_)\n",
    "    #add_graph(graph, entityNode, RDF.type, entitytype)\n",
    "    #add_graph(graph, entityNode, RDFS.label, Literal(ent.label_, lang=\"en\"))\n",
    "    #add_graph(graph, sentenceNode, ner, Literal(ent.text, lang=\"en\"))\n",
    "    #add_graph(graph, sentenceNode, nertype, entityNode)\n",
    "    #print(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Guardar tripletas en archivo sql\n",
    "def storeTriplesDBfile(grafo):\n",
    "    lengg = len(grafo)\n",
    "    iterador = 1\n",
    "    f =codecs.open(\"/home/ela/Dropbox/ProyectoUTPL-NLP/NLPcode_Lady/jupyterproy/tripletas/tripletasSinStopwords.sql\",\"wb\",encoding=\"UTF-8\")\n",
    "    for tripleta in grafo:\n",
    "        print (\"Guardando \" + str(iterador) + \" de \" + str(lengg))\n",
    "        print (tripleta)\n",
    "        sujeto1 = tripleta[0]\n",
    "        predicado1 = tripleta[1]\n",
    "        objeto1 = tripleta[2]\n",
    "        f.write('INSERT into tripletas values  (%s, %s, %s)' % ('\"' + sujeto1 + '\"', '\"' + predicado1 + '\"', '\"' + objeto1 + '\"') + ';\\n')\n",
    "        iterador +=1\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# buscar en la lista de ners guardados cada oracion que ingrese, si la encuentra es uu NER que debe guardarse\n",
    "def add_ner(doc, inicio, fin):\n",
    "    ner_name = doc.vocab.strings[u'MISC']  # get hash value of entity label\n",
    "    new_ent = Span(doc, inicio, fin, label=ner_name) # create a Span for the new entity\n",
    "    doc.ents = list(doc.ents) + [new_ent]\n",
    "    return doc.ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizing(doc,stoplist_punt):\n",
    "    solo_tokens=[]\n",
    "    for sent in doc.sents:\n",
    "        for token in sent:\n",
    "            if token.text not in stoplist_punt:\n",
    "                solo_tokens.append(token.text)      \n",
    "    return solo_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatizing(doc,stoplist_punt):\n",
    "    solo_lemas = []\n",
    "    for sent in doc.sents:\n",
    "        for token in sent:\n",
    "            #if token.lemma_ not in stoplist_lang:\n",
    "            if token.lemma_ not in stoplist_punt:\n",
    "                solo_lemas.append(token.lemma_)\n",
    "                #print(token.text, token.lemma_, token.pos_,token.is_stop)         \n",
    "    return solo_lemas"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
